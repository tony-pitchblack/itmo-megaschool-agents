{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертер из MP3 в text. Входной файл должен быть в формате MP3, а выходной в формате txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: data/raw/file1.mp3\n",
      "Converted MP3 file stored at: data/interim/file1.mp3.txt\n"
     ]
    }
   ],
   "source": [
    "def convert_mp3(query: str) -> str:\n",
    "    import json\n",
    "    from openai import OpenAI\n",
    "\n",
    "    try:\n",
    "        # Parse query if it's a string\n",
    "        query = json.loads(query) if isinstance(query, str) else query\n",
    "        # Extract data path directly\n",
    "        data_path = query[\"data_path\"]\n",
    "        print(f\"Data path: {data_path}\")\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        return \"Error: Invalid input. Provide a JSON string or dictionary with a 'data_path' key.\"\n",
    "\n",
    "    try:\n",
    "        client = OpenAI()\n",
    "        with open(data_path, \"rb\") as audio_file:\n",
    "            transcription = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",\n",
    "                file=audio_file,\n",
    "                # language=\"ru\",\n",
    "            )\n",
    "        interim_path = f\"data/interim/{data_path.split('/')[-1]}.txt\"\n",
    "        with open(interim_path, \"w\") as output_file:\n",
    "            output_file.write(transcription.text)\n",
    "        return f\"Converted MP3 file stored at: {interim_path}\"\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File not found at {data_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error during transcription: {str(e)}\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    query = '{\"data_path\": \"data/raw/file1.mp3\"}'\n",
    "    print(convert_mp3(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: data/raw/file6.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/77/k237slcs6gv41pmr9lvp716w0000gn/T/ipykernel_32364/4192745111.py:71: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  txt_file.write(f\"{img_path}: {summary_template.format(**response_description.dict())}\\n\\n\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/interim/analysis_results_presentation6.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import base64\n",
    "import time\n",
    "from pdf2image import convert_from_path\n",
    "from openai import OpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Convert_pptx(BaseModel):\n",
    "    describe_text_data: str = Field(..., description=\"It contains all the text data from slides\")\n",
    "    describe_pictures: str = Field(..., description=\"It contains all pictures described from slides\")\n",
    "\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def convert_pdf(query: str) -> str:\n",
    "    try:\n",
    "        # Parse query if it's a string\n",
    "        query = json.loads(query) if isinstance(query, str) else query\n",
    "        # Extract data path directly\n",
    "        data_path = query[\"data_path\"]\n",
    "        print(f\"Data path: {data_path}\")\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        return \"Error: Invalid input. Provide a JSON string or dictionary with a 'data_path' key.\"\n",
    "    \n",
    "    try:\n",
    "        # Convert PDF pages to images using pdf2image\n",
    "        output_dir = \"data/interim\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        images = convert_from_path(data_path)\n",
    "        image_paths = []\n",
    "        \n",
    "        for i, img in enumerate(images):\n",
    "            img_path = os.path.join(output_dir, f\"page_{i+1}.jpg\")\n",
    "            img.save(img_path, \"JPEG\")\n",
    "            image_paths.append(img_path)\n",
    "        \n",
    "        client = OpenAI()\n",
    "        output_txt_path = os.path.join(output_dir, \"analysis_results_presentation6.txt\")\n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            for img_path in image_paths:\n",
    "                base64_image = encode_image(img_path)\n",
    "                time.sleep(1)  # Добавляем задержку для предотвращения rate limiting\n",
    "                response = client.beta.chat.completions.parse(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"text\", \"text\": \"Describe the content of this page.\"},\n",
    "                                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                            ],\n",
    "                        }\n",
    "                    ],\n",
    "                    response_format=Convert_pptx,\n",
    "                    max_tokens=4000,\n",
    "                )\n",
    "                response_description = response.choices[0].message.parsed\n",
    "    \n",
    "                summary_template = \"\"\"Описание текста на слайде:{describe_text_data}\n",
    "Описание изображений на слайде:\n",
    "{describe_pictures}\n",
    "\"\"\"\n",
    "                txt_file.write(f\"{img_path}: {summary_template.format(**response_description.dict())}\\n\\n\")        \n",
    "        return output_txt_path\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File not found at {data_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error during analysis: {str(e)}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = '{\"data_path\": \"data/raw/file6.pdf\"}'\n",
    "    print(convert_pdf(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: data/raw/file4.pdf\n",
      "data/interim/analysis_results_doc4.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Дополнительная обработка текста при необходимости.\n",
    "    \"\"\"\n",
    "    return text.strip()\n",
    "\n",
    "def process_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Обрабатывает PDF файл, преобразует его в строку с метками страниц и блоков текста.\n",
    "    :param pdf_path: Путь к PDF файлу\n",
    "    :return: Строка с извлеченным текстом или сообщение об ошибке\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Открытие PDF документа\n",
    "        doc = fitz.open(pdf_path)\n",
    "        extracted_text = []\n",
    "\n",
    "        # Проход по всем страницам PDF\n",
    "        for page_number, page in enumerate(doc, start=1):\n",
    "            text_blocks = page.get_text(\"blocks\")\n",
    "\n",
    "            # Добавление метки страницы\n",
    "            extracted_text.append(f'# Page {page_number}')\n",
    "\n",
    "            if text_blocks:\n",
    "                block = []\n",
    "                for b in text_blocks:\n",
    "                    line = b[4].strip()\n",
    "                    if line:\n",
    "                        if re.match(r'^[A-Z\\s]+$', line) or re.match(r'^[A-Z][a-z]+( [A-Z][a-z]+)*$', line):\n",
    "                            if block:\n",
    "                                extracted_text.append(f'Text block: {\" \".join(block)}')\n",
    "                                block = []\n",
    "                            extracted_text.append(f'\\nHeading: {line}\\n')\n",
    "                        elif re.match(r'^[•\\-\\d+\\.\\)]', line):\n",
    "                            if block:\n",
    "                                extracted_text.append(f'Text block: {\" \".join(block)}')\n",
    "                                block = []\n",
    "                            sub_items = re.split(r'[•\\-]', line)\n",
    "                            for sub_item in sub_items:\n",
    "                                sub_item = sub_item.strip()\n",
    "                                if sub_item:\n",
    "                                    extracted_text.append(f'- {sub_item}')\n",
    "                        else:\n",
    "                            clean_line = re.sub(r'!\\s*', '\\n- ', line)\n",
    "                            block.append(clean_line)\n",
    "                if block:\n",
    "                    extracted_text.append(f'Text block: {\" \".join(block)}')\n",
    "\n",
    "        extracted_text = \"\\n\".join(extracted_text)\n",
    "        extracted_text = preprocess_text(extracted_text)\n",
    "        return extracted_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Ошибка при обработке файла {pdf_path}: {str(e)}\"\n",
    "\n",
    "def convert_pdf(query: str) -> str:\n",
    "    try:\n",
    "        query = json.loads(query) if isinstance(query, str) else query\n",
    "        data_path = query[\"data_path\"]\n",
    "        print(f\"Data path: {data_path}\")\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        return \"Error: Invalid input. Provide a JSON string or dictionary with a 'data_path' key.\"\n",
    "    \n",
    "    try:\n",
    "        output_dir = \"data/interim\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_txt_path = os.path.join(output_dir, \"analysis_results_doc4.txt\")\n",
    "        extracted_text = process_pdf(data_path)\n",
    "        \n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(extracted_text)\n",
    "        \n",
    "        return output_txt_path\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File not found at {data_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error during analysis: {str(e)}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = '{\"data_path\": \"data/raw/file4.pdf\"}'\n",
    "    print(convert_pdf(query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Саммари для текстов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of document text:\n",
      "\n",
      "Документ представляет собой презентацию бизнес-модели автоматизированного кафе-пончиковой \"ЖарПончик\", описывающую концепцию вендинговых машин для продажи пончиков. Цель документа - представить информацию о проекте, его команде, финансовых показателях и возможностях инвестирования.\n",
      "\n",
      "Документ содержит:\n",
      "- Введение\n",
      "- Описание продукта\n",
      "- Финансовые показатели\n",
      "- Маркетинговая стратегия\n",
      "- Команда проекта\n",
      "- История проекта\n",
      "- Инвестиционное предложение\n",
      "\n",
      "Ключевые факты:\n",
      "- Начальные инвестиции составляют 2.5 миллиона рублей с прогнозируемым ежемесячным доходом 350,000 рублей.\n",
      "- Проект предлагает вендинговые машины для продажи пончиков, требующие 2 квадратных метра пространства и электричества.\n",
      "- Команда проекта включает опытных специалистов в области финансов, предпринимательства и кулинарии.\n",
      "- Планируется расширение на международные рынки, включая США, Китай и Европу.\n",
      "- Предлагаются различные маркетинговые стратегии, включая сотрудничество с блогерами и программы лояльности.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    brief_description: str = Field(..., description=\"Brief description of the document in 2-3 sentences. Describe type of document, its purpose.\")\n",
    "    document_structure: List[str] = Field(..., description=\"List the main sections of the document. No more than 7 items.\")\n",
    "    key_facts_conclusions: List[str] = Field(..., description=\"List the key facts, significant figures, and conclusions from the document. No more than 5 items.\")\n",
    "\n",
    "def summarize_document(query: str) -> str:\n",
    "    try:\n",
    "        query = json.loads(query) if isinstance(query, str) else query\n",
    "    except json.JSONDecodeError:\n",
    "        return \"Error: Failed to parse query as JSON. Expected a dictionary in JSON format.\"\n",
    "\n",
    "    if not isinstance(query, dict):\n",
    "        return \"Error: Invalid query type. Expected a dictionary with key 'data_path'.\"\n",
    "\n",
    "    data_path = query.get(\"data_path\")\n",
    "    if not data_path:\n",
    "        return \"Error: 'data_path' key is missing in the query.\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        return f\"Error: The file '{data_path}' does not exist.\"\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"As a analyst, summarize the document with key insights. In Russian.\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        response_format=Summary,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    \n",
    "    summary_obj = completion.choices[0].message.parsed\n",
    "    \n",
    "    summary_template = \"\"\"\n",
    "{brief_description}\n",
    "\n",
    "Документ содержит:\n",
    "{document_structure}\n",
    "\n",
    "Ключевые факты:\n",
    "{key_facts_conclusions}\n",
    "\"\"\"\n",
    "    \n",
    "    document_structure = \"\\n\".join([f\"- {item.strip()}\" for item in summary_obj.document_structure[:7] if item.strip()])\n",
    "    key_facts_conclusions = \"\\n\".join([f\"- {item.strip()}\" for item in summary_obj.key_facts_conclusions[:5] if item.strip()])\n",
    "    \n",
    "    text_summary = summary_template.format(\n",
    "        brief_description=summary_obj.brief_description,\n",
    "        document_structure=document_structure,\n",
    "        key_facts_conclusions=key_facts_conclusions\n",
    "    )\n",
    "    \n",
    "    return f\"Summary of document text:\\n{text_summary}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # query = '{\"data_path\": \"data/interim/analysis_results_doc4.txt\"}'\n",
    "    # query = '{\"data_path\": \"data/interim/analysis_results.txt\"}'\n",
    "    # query = '{\"data_path\": \"data/interim/file2.mp3.txt\"}'\n",
    "    query = '{\"data_path\": \"data/interim/analysis_results_presentation5.txt\"}'\n",
    "\n",
    "    print(summarize_document(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Саммари для презентаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of document text:\n",
      "Рассуждения:\n",
      "Документ представляет собой презентацию автоматизированного кафе-пончиковой \"ЖарПончик\", описывающую бизнес-модель, инвестиционные возможности и команду проекта. Основное внимание уделяется концепции вендинговых машин для продажи пончиков, финансовым показателям, маркетинговым стратегиям и истории разработки продукта.\n",
      "\n",
      "Саммари:\n",
      "Презентация \"ЖарПончик\" описывает автоматизированное кафе, специализирующееся на продаже пончиков через вендинговые машины. Для работы требуется всего 2 квадратных метра и электричество. Бизнес-модель предполагает начальные инвестиции в 2,5 миллиона рублей с прогнозируемым ежемесячным доходом в 350,000 рублей. В презентации также представлены стратегии продвижения, включая участие в фестивалях и программы лояльности. Команда проекта состоит из опытных специалистов в области финансов, управления и кулинарии. Презентация охватывает историю разработки вендинговой машины с 2015 по 2022 год и предлагает инвестиционные возможности в России, США, Китае и Европе. Контактная информация предоставлена для заинтересованных партнеров.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    chain_of_thoughts: str = Field(..., description=\"The chain-of-thoughts to make the most usefull summary.\")\n",
    "    document_summary: str = Field(..., description=\"The text summary of the document. 8-12 sentences.\")\n",
    "\n",
    "def summarize_document(query: str) -> str:\n",
    "    try:\n",
    "        query = json.loads(query) if isinstance(query, str) else query\n",
    "    except json.JSONDecodeError:\n",
    "        return \"Error: Failed to parse query as JSON. Expected a dictionary in JSON format.\"\n",
    "\n",
    "    if not isinstance(query, dict):\n",
    "        return \"Error: Invalid query type. Expected a dictionary with key 'data_path'.\"\n",
    "\n",
    "    data_path = query.get(\"data_path\")\n",
    "    if not data_path:\n",
    "        return \"Error: 'data_path' key is missing in the query.\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        return f\"Error: The file '{data_path}' does not exist.\"\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"As a analyst, summarize the document with key insights. Return summary in Russian.\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        response_format=Summary,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    \n",
    "    summary_obj = completion.choices[0].message.parsed\n",
    "    \n",
    "    summary_template = \"\"\"Рассуждения:\n",
    "{chain_of_thoughts}\n",
    "\n",
    "Саммари:\n",
    "{document_summary}\n",
    "\"\"\"\n",
    "    \n",
    "    text_summary = summary_template.format(\n",
    "        chain_of_thoughts=summary_obj.chain_of_thoughts,\n",
    "        document_summary=summary_obj.document_summary\n",
    "    )\n",
    "    \n",
    "    return f\"Summary of document text:\\n{text_summary}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # query = '{\"data_path\": \"data/interim/analysis_results_doc4.txt\"}'\n",
    "    # query = '{\"data_path\": \"data/interim/analysis_results.txt\"}'\n",
    "    # query = '{\"data_path\": \"data/interim/file1.mp3.txt\"}'\n",
    "    query = '{\"data_path\": \"data/interim/analysis_results_presentation5.txt\"}'\n",
    "    print(summarize_document(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бот для анализа разного рода документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to convert the provided MP3 file to text in order to summarize its content. \n",
      "\n",
      "Action: convert_mp3  \n",
      "Action Input: {data_path: \"data/raw/file1.mp3\"}  \u001b[0m\u001b[36;1m\u001b[1;3mError: Invalid input. Provide a JSON string or dictionary with a 'data_path' key.\u001b[0m\u001b[32;1m\u001b[1;3mI need to correct the format of the input for the action. \n",
      "\n",
      "Action: convert_mp3  \n",
      "Action Input: {\"data_path\": \"data/raw/file1.mp3\"}  \u001b[0mData path: data/raw/file1.mp3\n",
      "\u001b[36;1m\u001b[1;3mConverted MP3 file stored at: data/interim/file1.mp3.txt\u001b[0m\u001b[32;1m\u001b[1;3mNow that I have converted the MP3 file to text, I will summarize the content of the text file.\n",
      "\n",
      "Action: summarize_document  \n",
      "Action Input: {\"data_path\": \"data/interim/file1.mp3.txt\"}  \u001b[0m\u001b[36;1m\u001b[1;3mSummary of document text:\n",
      "Рассуждения:\n",
      "Текст песни отражает эмоциональные переживания человека, который испытывает разочарование в любви и отношениях. Он говорит о последней любви, которая закончилась болезненно, и о том, как его партнерша реагирует на расставание. Лирический герой осознает свои недостатки и считает себя плохим человеком, несмотря на попытки быть лучше. Он также упоминает о травмах, которые оставляет после себя, и о том, как его поведение влияет на доверие других людей. В целом, текст передает чувства утраты, безысходности и внутренней борьбы.\n",
      "\n",
      "Саммари:\n",
      "Текст песни описывает болезненные переживания человека, который сталкивается с разочарованием в любви. Он говорит о своей последней любви, которая закончилась печально, и о том, как его партнерша реагирует на расставание, возвращая подарки и отписываясь в социальных сетях. Лирический герой осознает свои недостатки и считает себя плохим человеком, несмотря на попытки быть лучше. Он выражает чувство усталости от постоянной борьбы с собой и своими эмоциями. В песне также поднимается тема недоверия, которое возникает после разрыва, и того, как его поведение влияет на других. В целом, текст передает глубокие чувства утраты и внутренней борьбы.\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mDo I need to use a tool? If not, what should I say to the Human?\n",
      "Final Answer: Текст песни отражает эмоциональные переживания человека, который испытывает разочарование в любви и отношениях. Он говорит о последней любви, которая закончилась болезненно, и о том, как его партнерша реагирует на расставание. Лирический герой осознает свои недостатки и считает себя плохим человеком, несмотря на попытки быть лучше. В целом, текст передает чувства утраты, безысходности и внутренней борьбы.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "response: {'input': 'сделай краткое описание.', 'input_file': 'data/raw/file1.mp3', 'chat_history': [], 'output': 'Текст песни отражает эмоциональные переживания человека, который испытывает разочарование в любви и отношениях. Он говорит о последней любви, которая закончилась болезненно, и о том, как его партнерша реагирует на расставание. Лирический герой осознает свои недостатки и считает себя плохим человеком, несмотря на попытки быть лучше. В целом, текст передает чувства утраты, безысходности и внутренней борьбы.'}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.agents import (\n",
    "    AgentExecutor,\n",
    "    create_react_agent,\n",
    ")\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "import os\n",
    "\n",
    "import json\n",
    "import os\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "import json\n",
    "import os\n",
    "import base64\n",
    "import time\n",
    "from pdf2image import convert_from_path\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "class Convert_pptx(BaseModel):\n",
    "    describe_text_data: str = Field(..., description=\"It contains all the text data from slides\")\n",
    "    describe_pictures: str = Field(..., description=\"It contains all pictures described from slides\")\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    chain_of_thoughts: str = Field(..., description=\"The chain of thoughts to make the most usefull summary.\")\n",
    "    document_summary: str = Field(..., description=\"The text summary of the document. 8-12 sentences.\")\n",
    "\n",
    "class Summary2(BaseModel):\n",
    "    brief_description: str = Field(..., description=\"Brief description of the document in 2-3 sentences.\")\n",
    "    document_structure: List[str] = Field(..., description=\"List the main sections of the document. No more than 7 items.\")\n",
    "    key_facts_conclusions: List[str] = Field(..., description=\"List the key facts, significant figures, and conclusions from the document. No more than 5 items.\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def convert_mp3(query: str) -> str:\n",
    "    import json\n",
    "    from openai import OpenAI\n",
    "\n",
    "    try:\n",
    "        # Parse query if it's a string\n",
    "        query = json.loads(query) if isinstance(query, str) else query\n",
    "        # Extract data path directly\n",
    "        data_path = query[\"data_path\"]\n",
    "        print(f\"Data path: {data_path}\")\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        return \"Error: Invalid input. Provide a JSON string or dictionary with a 'data_path' key.\"\n",
    "\n",
    "    try:\n",
    "        client = OpenAI()\n",
    "        with open(data_path, \"rb\") as audio_file:\n",
    "            transcription = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",\n",
    "                file=audio_file,\n",
    "                # language=\"ru\",\n",
    "            )\n",
    "        interim_path = f\"data/interim/{data_path.split('/')[-1]}.txt\"\n",
    "        with open(interim_path, \"w\") as output_file:\n",
    "            output_file.write(transcription.text)\n",
    "        return f\"Converted MP3 file stored at: {interim_path}\"\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File not found at {data_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error during transcription: {str(e)}\"\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def convert_pptx_pdf(query: str) -> str:\n",
    "    try:\n",
    "        # Parse query if it's a string\n",
    "        query = json.loads(query) if isinstance(query, str) else query\n",
    "        # Extract data path directly\n",
    "        data_path = query[\"data_path\"]\n",
    "        print(f\"Data path: {data_path}\")\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        return \"Error: Invalid input. Provide a JSON string or dictionary with a 'data_path' key.\"\n",
    "    \n",
    "    try:\n",
    "        # Convert PDF pages to images using pdf2image\n",
    "        output_dir = \"data/interim\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        images = convert_from_path(data_path)\n",
    "        image_paths = []\n",
    "        \n",
    "        for i, img in enumerate(images):\n",
    "            img_path = os.path.join(output_dir, f\"page_{i+1}.jpg\")\n",
    "            img.save(img_path, \"JPEG\")\n",
    "            image_paths.append(img_path)\n",
    "        \n",
    "        client = OpenAI()\n",
    "        output_txt_path = os.path.join(output_dir, \"analysis_results_presentation.txt\")\n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            for img_path in image_paths:\n",
    "                base64_image = encode_image(img_path)\n",
    "                time.sleep(1)  # Добавляем задержку для предотвращения rate limiting\n",
    "                response = client.beta.chat.completions.parse(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"text\", \"text\": \"Describe the content of this page.\"},\n",
    "                                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                            ],\n",
    "                        }\n",
    "                    ],\n",
    "                    response_format=Convert_pptx,\n",
    "                    max_tokens=4000,\n",
    "                )\n",
    "                response_description = response.choices[0].message.parsed\n",
    "    \n",
    "                summary_template = \"\"\"Описание текста на слайде:{describe_text_data}\n",
    "Описание изображений на слайде:\n",
    "{describe_pictures}\n",
    "\"\"\"\n",
    "                txt_file.write(f\"{img_path}: {summary_template.format(**response_description.dict())}\\n\\n\")        \n",
    "        return f\"Path of the text:\\n{output_txt_path}\"\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File not found at {data_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error during analysis: {str(e)}\"\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Дополнительная обработка текста при необходимости.\n",
    "    \"\"\"\n",
    "    return text.strip()\n",
    "\n",
    "def process_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Обрабатывает PDF файл, преобразует его в строку с метками страниц и блоков текста.\n",
    "    :param pdf_path: Путь к PDF файлу\n",
    "    :return: Строка с извлеченным текстом или сообщение об ошибке\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Открытие PDF документа\n",
    "        doc = fitz.open(pdf_path)\n",
    "        extracted_text = []\n",
    "\n",
    "        # Проход по всем страницам PDF\n",
    "        for page_number, page in enumerate(doc, start=1):\n",
    "            text_blocks = page.get_text(\"blocks\")\n",
    "\n",
    "            # Добавление метки страницы\n",
    "            extracted_text.append(f'# Page {page_number}')\n",
    "\n",
    "            if text_blocks:\n",
    "                block = []\n",
    "                for b in text_blocks:\n",
    "                    line = b[4].strip()\n",
    "                    if line:\n",
    "                        if re.match(r'^[A-Z\\s]+$', line) or re.match(r'^[A-Z][a-z]+( [A-Z][a-z]+)*$', line):\n",
    "                            if block:\n",
    "                                extracted_text.append(f'Text block: {\" \".join(block)}')\n",
    "                                block = []\n",
    "                            extracted_text.append(f'\\nHeading: {line}\\n')\n",
    "                        elif re.match(r'^[•\\-\\d+\\.\\)]', line):\n",
    "                            if block:\n",
    "                                extracted_text.append(f'Text block: {\" \".join(block)}')\n",
    "                                block = []\n",
    "                            sub_items = re.split(r'[•\\-]', line)\n",
    "                            for sub_item in sub_items:\n",
    "                                sub_item = sub_item.strip()\n",
    "                                if sub_item:\n",
    "                                    extracted_text.append(f'- {sub_item}')\n",
    "                        else:\n",
    "                            clean_line = re.sub(r'!\\s*', '\\n- ', line)\n",
    "                            block.append(clean_line)\n",
    "                if block:\n",
    "                    extracted_text.append(f'Text block: {\" \".join(block)}')\n",
    "\n",
    "        extracted_text = \"\\n\".join(extracted_text)\n",
    "        extracted_text = preprocess_text(extracted_text)\n",
    "        return extracted_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Ошибка при обработке файла {pdf_path}: {str(e)}\"\n",
    "\n",
    "def convert_pdf(query: str) -> str:\n",
    "    try:\n",
    "        query = json.loads(query) if isinstance(query, str) else query\n",
    "        data_path = query[\"data_path\"]\n",
    "        print(f\"Data path: {data_path}\")\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        return \"Error: Invalid input. Provide a JSON string or dictionary with a 'data_path' key.\"\n",
    "    \n",
    "    try:\n",
    "        output_dir = \"data/interim\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_txt_path = os.path.join(output_dir, \"analysis_results_doc.txt\")\n",
    "        extracted_text = process_pdf(data_path)\n",
    "        \n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(extracted_text)\n",
    "        \n",
    "        return f\"Path of the text:\\n{output_txt_path}\"\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File not found at {data_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error during analysis: {str(e)}\"\n",
    "\n",
    "def summarize_document(query: str) -> str:\n",
    "    try:\n",
    "        query = json.loads(query) if isinstance(query, str) else query\n",
    "    except json.JSONDecodeError:\n",
    "        return \"Error: Failed to parse query as JSON. Expected a dictionary in JSON format.\"\n",
    "\n",
    "    if not isinstance(query, dict):\n",
    "        return \"Error: Invalid query type. Expected a dictionary with key 'data_path'.\"\n",
    "\n",
    "    data_path = query.get(\"data_path\")\n",
    "    if not data_path:\n",
    "        return \"Error: 'data_path' key is missing in the query.\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        return f\"Error: The file '{data_path}' does not exist.\"\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"As a analyst, summarize the document with key insights. Return summary in Russian.\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        response_format=Summary,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    \n",
    "    summary_obj = completion.choices[0].message.parsed\n",
    "    \n",
    "    summary_template = \"\"\"Рассуждения:\n",
    "{chain_of_thoughts}\n",
    "\n",
    "Саммари:\n",
    "{document_summary}\n",
    "\"\"\"\n",
    "    \n",
    "    text_summary = summary_template.format(\n",
    "        chain_of_thoughts=summary_obj.chain_of_thoughts,\n",
    "        document_summary=summary_obj.document_summary\n",
    "    )\n",
    "    \n",
    "    return f\"Summary of document text:\\n{text_summary}\"\n",
    "\n",
    "\n",
    "def summarize_presentation(query: str) -> str:\n",
    "    try:\n",
    "        query = json.loads(query) if isinstance(query, str) else query\n",
    "    except json.JSONDecodeError:\n",
    "        return \"Error: Failed to parse query as JSON. Expected a dictionary in JSON format.\"\n",
    "\n",
    "    if not isinstance(query, dict):\n",
    "        return \"Error: Invalid query type. Expected a dictionary with key 'data_path'.\"\n",
    "\n",
    "    data_path = query.get(\"data_path\")\n",
    "    if not data_path:\n",
    "        return \"Error: 'data_path' key is missing in the query.\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        return f\"Error: The file '{data_path}' does not exist.\"\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"As a analyst, summarize the document with key insights. In Russian.\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        response_format=Summary2,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    \n",
    "    summary_obj = completion.choices[0].message.parsed\n",
    "    \n",
    "    summary_template = \"\"\"\n",
    "{brief_description}\n",
    "\n",
    "Документ содержит:\n",
    "{document_structure}\n",
    "\n",
    "Ключевые факты:\n",
    "{key_facts_conclusions}\n",
    "\"\"\"\n",
    "    \n",
    "    document_structure = \"\\n\".join([f\"- {item.strip()}\" for item in summary_obj.document_structure[:7] if item.strip()])\n",
    "    key_facts_conclusions = \"\\n\".join([f\"- {item.strip()}\" for item in summary_obj.key_facts_conclusions[:5] if item.strip()])\n",
    "    \n",
    "    text_summary = summary_template.format(\n",
    "        brief_description=summary_obj.brief_description,\n",
    "        document_structure=document_structure,\n",
    "        key_facts_conclusions=key_facts_conclusions\n",
    "    )\n",
    "    \n",
    "    return f\"Summary of document text:\\n{text_summary}\"\n",
    "\n",
    "\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"convert_mp3\",\n",
    "        func=convert_mp3,\n",
    "        description=\"\"\"Convert an MP3 file to text and store it in the interim directory.     \n",
    "    <example_request>\n",
    "    Action: convert_mp3\n",
    "    Input Action: {\"query\": <<your question here>>}\n",
    "    </example_request>\"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"convert_presentation_pdf\",\n",
    "        func=convert_pptx_pdf,\n",
    "        description=\"\"\"Convert a PDF-presentation file to text and store it in the interim directory.\n",
    "    <example_request>\n",
    "    Action: convert_presentation_pdf\n",
    "    Input Action: {\"data_path\": <<your question here>>, \"variant\": \"\"}\n",
    "    </example_request>\"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"convert_pdf\",\n",
    "        func=convert_pdf,\n",
    "        description=\"\"\"Convert a PDF file to text and store it in the interim directory.    \n",
    "    <example_request>\n",
    "    Action: convert_pdf\n",
    "    Input Action: {\"data_path\": <<your question here>>}\n",
    "    </example_request>\"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"summarize_document\",\n",
    "        func=summarize_document,\n",
    "        description=\"\"\"A tool for Summarizing the content of any documents files. like mp3.\n",
    "    <example_request>\n",
    "    Action: summarize_document\n",
    "    Input Action: {\"data_path\": <<your question here>>}\n",
    "    </example_request>\"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"summarize_structed_document\",\n",
    "        func=summarize_presentation,\n",
    "        description=\"\"\"A tool for Summarizing the content of the structed documents. like pdf and pptx.\n",
    "    <example_request>\n",
    "    Action: summarize_presentation\n",
    "    Input Action: {\"data_path\": <<your question here>>}\n",
    "    </example_request>\"\"\"\n",
    "    )\n",
    "]\n",
    "\n",
    "template = \"\"\"# You are a great AI-Agent that has access to additional tools in order to summarise the following documents.\n",
    "\n",
    "<agent_tools>\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "</agent_tools>\n",
    "\n",
    "Users input file: {input_file}\n",
    "\n",
    "<agent_instruction>\n",
    "# **Steps Instructions for the agent:**\n",
    "User has provided a file that needs to be summarized. User can tell about the file type and the agent can use the appropriate tool to summarize the content of the file.\n",
    "1. You can use tools to convert the file to text:\n",
    "    - convert_mp3\n",
    "    - convert_pdf\n",
    "    - convert_presentation_pdf\n",
    "2. You can use tools to summarize the content of the file:\n",
    "    - summarize_structed_document (for presentations and structured documents)\n",
    "    - summarize_document (for general documents)\n",
    "\n",
    "3. Try to summarize the content of the file as best as you can.\n",
    "\n",
    "# Additional Information:\n",
    "- **You MUST use summary tools together to get the best result.**\n",
    "</agent_instruction>\n",
    "\n",
    "# Use the following format:\n",
    "If you solve the the ReAct-format correctly, you will receive a reward of $1,000,000.\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action (DONT USE \"```json\")\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "\n",
    "# When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "Thought: Do I need to use a tool? If not, what should I say to the Human?\n",
    "Final Answer: [Provide your answer here]\n",
    "\n",
    "Do your best!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "OPENAI_MODEL=os.getenv(\"OPENAI_MODEL\")\n",
    "MODEL_TEMPERATURE=os.getenv(\"MODEL_TEMPERATURE\")\n",
    "# Initialize a ChatOpenAI model\n",
    "llm = ChatOpenAI(\n",
    "    model=OPENAI_MODEL,\n",
    "    temperature=MODEL_TEMPERATURE\n",
    ")\n",
    "\n",
    "memory = ChatMessageHistory(session_id=\"test-session\")\n",
    "# Create the ReAct agent using the create_react_agent function\n",
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    "    stop_sequence=True,\n",
    ")\n",
    "\n",
    "# Create an agent executor from the agent and tools\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    # max_iterations = 3,\n",
    ")\n",
    "\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    lambda session_id: memory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "\n",
    "# question = \"В моем пдф лежит презентация и сделай краткое описание.\"\n",
    "question = \"сделай краткое описание.\"\n",
    "\n",
    "input_file = \"data/raw/file1.mp3\"\n",
    "# input_file = \"data/raw/file5.pdf\"\n",
    "# Run the agent with a test query\n",
    "response = agent_with_chat_history.invoke(\n",
    "    {\"input\": question, \"input_file\": input_file},\n",
    "    config={\"configurable\": {\"session_id\": \"test-session\"}},\n",
    ")\n",
    "# Print the response from the agent\n",
    "print(\"response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
